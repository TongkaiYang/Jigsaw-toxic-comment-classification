{"cells":[{"metadata":{"id":"tD2OlHHh34mB","outputId":"4bf81983-6a8a-44f4-f8b3-6ee520df3b9b","trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nimport os\nfrom tqdm.notebook import tqdm\nimport transformers\nimport tokenizers","execution_count":null,"outputs":[]},{"metadata":{"id":"KWPUz8Uz4fIY"},"cell_type":"markdown","source":"## **TPU Setting**"},{"metadata":{"id":"p0uLsa494LKo","outputId":"3b1dc349-e1bc-45fd-f6a8-954dfbbb8436","trusted":true},"cell_type":"code","source":"resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\nnum_tpu = strategy.num_replicas_in_sync\nprint('Number of TPU:', num_tpu)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Load Data**"},{"metadata":{"id":"3gvP-ry44NC-","outputId":"a8ef97f4-e827-437d-f222-d2720d834161","trusted":true},"cell_type":"code","source":"#Load data\nTRAIN_PATH1 = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv'\nTRAIN_PATH2 = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv'\nVAL_PATH = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv'\nTEST_PATH = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv'\ncomment_train1 = pd.read_csv(TRAIN_PATH1)\ncomment_train2 = pd.read_csv(TRAIN_PATH2)\ncomment_train2['toxic'] = comment_train2['toxic'].round().astype(int)\n\n#add more data with lebel 1 to training data\ncomment_train = pd.concat([\n    comment_train1[['comment_text', 'toxic']],\n    comment_train2[['comment_text', 'toxic']].query('toxic == 1'),\n    comment_train2[['comment_text', 'toxic']].query('toxic == 0').sample(100000, random_state=111)\n])\n\ncomment_train = comment_train.sample(frac=1)\ncomment_val = pd.read_csv(VAL_PATH)\ncomment_test = pd.read_csv(TEST_PATH)","execution_count":null,"outputs":[]},{"metadata":{"id":"GVRlf0ji4UdY"},"cell_type":"markdown","source":"## **Data Processing**"},{"metadata":{"id":"RSwe6K_k4RfC","trusted":true},"cell_type":"code","source":"#funtion to clean comments in training data\nimport re\ndef comment_clean(text):\n    text = re.sub(\"\\\\n\", \" \", text)\n    #remove digits\n    text = re.sub(\"\\d\", \"\", text)\n    #remove user_id and IP address\n    text = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", '', text)\n    #remove text staring with user\n    text = re.sub(\"\\[\\[User.*\",'', text)\n\n    #remove url and links\n    text = re.sub('^((https?|ftp|smtp):\\/\\/)?(www.)?[a-z0-9]+\\.[a-z]+(\\/[a-zA-Z0-9#]+\\/?)*$', ' ', text)\n    clean_text = ''\n    for char in text.strip():\n      #remove special characters and punctuations\n      if char.isprintable() and not char.isnumeric():\n        clean_text += char\n    return clean_text","execution_count":null,"outputs":[]},{"metadata":{"id":"7_zlYIXL4cpv","trusted":true},"cell_type":"code","source":"comment_train['comment_text'] = comment_train['comment_text'].apply(comment_clean)\ncomment_val['comment_text'] = comment_val['comment_text'].apply(comment_clean)\ncomment_test['content'] = comment_test['content'].apply(comment_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#add text tokenizer\ndef text_tokenize(texts, tokenizer, max_len):\n    encode = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=max_len\n    )\n    \n    return np.array(encode['input_ids'])","execution_count":null,"outputs":[]},{"metadata":{"id":"6Ef2eTss4xgu","trusted":true},"cell_type":"code","source":"x_train = comment_train['comment_text']\ny_train = np.array([toxic for toxic in comment_train['toxic']])\n\nx_val = comment_val['comment_text']\ny_val = np.array([toxic for toxic in comment_val['toxic']])\n\nx_test = comment_test['content']","execution_count":null,"outputs":[]},{"metadata":{"id":"ajK3d9uD4yQp","trusted":true},"cell_type":"code","source":"#load tokenizer\nxlm_roberta_tokenizer = transformers.AutoTokenizer.from_pretrained('xlm-roberta-base')","execution_count":null,"outputs":[]},{"metadata":{"id":"Jcx4iRlI43Qg","outputId":"e3480fb3-6d8c-40d0-d79e-3fba3d01b360","trusted":true},"cell_type":"code","source":"#tokenize data\nx_train_tokenized = text_tokenize(x_train, xlm_roberta_tokenizer, max_len=128)\nx_val_tokenized = text_tokenize(x_val, xlm_roberta_tokenizer, max_len=128)\ntest_tokenized = text_tokenize(x_test, xlm_roberta_tokenizer, max_len=128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create tensorflow dataset\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train_tokenized, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(-1)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_val_tokenized, y_val))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(-1)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZbBM83qFgnDv","trusted":true},"cell_type":"code","source":"from tensorflow.keras import backend as K\ndef focal_loss(gamma=2., alpha=.2):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n    return focal_loss_fixed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback, LearningRateScheduler\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nclass RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#learning rate scheduler\ndef build_lrfn(lr_start=0.000001, lr_max=0.000002, \n               lr_min=0.0000001, lr_rampup_epochs=7, \n               lr_sustain_epochs=0, lr_exp_decay=.87):\n    lr_max = lr_max * 8\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create customized callback\ndef callback():\n    cb = []\n    reduceLROnPlat = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',  \n                                    factor=0.3, patience=3, \n                                    verbose=1, mode='auto', \n                                    epsilon=0.0001, cooldown=1, min_lr=0.000001)\n    cb.append(reduceLROnPlat)\n    RocAuc = RocAucEvaluation(validation_data=(x_val_tokenized, y_val), interval=1)\n    cb.append(RocAuc)\n    lrfn = build_lrfn()\n    lr_callback = LearningRateScheduler(lrfn, verbose=1)\n    cb.append(lr_callback)\n    return cb","execution_count":null,"outputs":[]},{"metadata":{"id":"SCzYNvZ2Y9Zu","trusted":true},"cell_type":"code","source":"#build model\ndef create_model(max_len, transformer, loss):\n    inputs = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"inputs\")\n    bert_outputs = transformer(inputs)[0]\n    class_token = bert_outputs[:,0,:]\n    dropout = tf.keras.layers.Dropout(0.3)(class_token)\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-5), loss=loss, metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"Bk3M4kOaR3N0","outputId":"f434b07e-3d57-4054-d522-c7117d454a98","trusted":true},"cell_type":"code","source":"#load xlm-roberta transfomer and initiate model\nwith strategy.scope():\n    xlmroberta_model = transformers.TFAutoModel.from_pretrained('jplu/tf-xlm-roberta-large')\n    model = create_model(128, xlmroberta_model, loss=focal_loss())\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"7ZBqP_U8dbd3","outputId":"e5845b84-ee01-4691-8af8-418a3b836c7a","trusted":true},"cell_type":"code","source":"#fit the model\nhistory = model.fit(train_dataset, validation_data=valid_dataset, epochs=3, \n                    steps_per_epoch=x_train.shape[0] // BATCH_SIZE, callbacks=callback())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit on validation data\nhistory_on_val = model.fit(valid_dataset.repeat(), epochs=8, steps_per_epoch=x_val.shape[0] // BATCH_SIZE)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}